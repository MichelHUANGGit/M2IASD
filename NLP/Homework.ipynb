{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The first cells of the notebook are the same as in the TP on text convolution. Apply the same preprocessing to get a dataset (with the same tokenizer) with a train and a validation split, with two columns review_ids (list of int) and label (int)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER** : Copying what we did in the TP on text convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from tabulate import tabulate\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version de pytorch :  2.2.0+cu121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Version de pytorch : \", torch.__version__)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['review', 'sentiment'],\n",
      "    num_rows: 50000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"scikit-learn/imdb\", split=\"train\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of the tokenizer: <class 'collections.OrderedDict'>\n",
      "Length of the vocabulary: 30522\n",
      "OrderedDict({'[PAD]': 0, '[unused0]': 1, '[unused1\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of the tokenizer:\", type(tokenizer.vocab))\n",
    "VOCSIZE = len(tokenizer.vocab)\n",
    "print(\"Length of the vocabulary:\", VOCSIZE)\n",
    "print(str(tokenizer.vocab)[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_fn(x, tokenizer):\n",
    "    x[\"review_ids\"] = tokenizer(\n",
    "        x[\"review\"],\n",
    "        add_special_tokens=False,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=False,\n",
    "    )[\"input_ids\"]\n",
    "    x[\"label\"] = 0 if x[\"sentiment\"] == \"negative\" else 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 5000  # the number of training example\n",
    "\n",
    "# We first shuffle the data !\n",
    "dataset = dataset.shuffle(seed=0)\n",
    "\n",
    "# Select 5000 samples\n",
    "sampled_dataset = dataset.select(range(n_samples))\n",
    "\n",
    "# Tokenize the dataset\n",
    "sampled_dataset = sampled_dataset.map(\n",
    "    preprocessing_fn, fn_kwargs={\"tokenizer\" : tokenizer}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove useless columns\n",
    "sampled_dataset = sampled_dataset.select_columns(['review_ids','label'])\n",
    "\n",
    "# Split the train and validation\n",
    "splitted_dataset = sampled_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "document_train_set = splitted_dataset['train']\n",
    "document_valid_set = splitted_dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write a function extract_words_contexts. It should retrieve all pairs of valid $(w, C^+)$ from a list of ids representing a text document. It takes the radius $R$ as an argument. Its output is therefore two lists :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to make sure that every C has the same size, we add padding at the beginning and the end of the sentence. For example the first word of the sentence, will have R paddings corresponding to the R tokens that should be before. We can also use the token itself, so that it has a high dot product with itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words_contexts(sample, R):\n",
    "    token_ids = sample[\"review_ids\"]\n",
    "    n_tokens = len(token_ids)\n",
    "    positive_context = []\n",
    "    token_ids_with_padding = [0]*R + token_ids + [0]*R\n",
    "    for i in range(n_tokens) :\n",
    "        # if out of bounds\n",
    "        if i<R or i>=n_tokens-R :\n",
    "            positive_context.append([token_ids_with_padding[i+r] for r in range(R)] + [token_ids_with_padding[i+R+r] for r in range(1,R+1, 1)])\n",
    "        else :\n",
    "            positive_context.append([token_ids[i+r] for r in range(-R, 0, 1)] + [token_ids[i+r] for r in range(1, R+1, 1)])\n",
    "    return token_ids, positive_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "toto, test = extract_words_contexts(document_train_set[2], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 tokens : [1045, 3856, 2039, 2023, 3185]\n",
      "C+ of the first 5 tokens :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 3856, 2039, 2023],\n",
       " [0, 0, 1045, 2039, 2023, 3185],\n",
       " [0, 1045, 3856, 2023, 3185, 1998],\n",
       " [1045, 3856, 2039, 3185, 1998, 2001],\n",
       " [3856, 2039, 2023, 1998, 2001, 14603]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"First 5 tokens :\", toto[:5])\n",
    "print(\"C+ of the first 5 tokens :\")\n",
    "test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write a function flatten_dataset_to_list that applies the function extract_words_contexts on a whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dataset_to_list(dataset, R):\n",
    "    token_ids = []\n",
    "    positive_contexts = []\n",
    "    for sample in dataset:\n",
    "        sample_token_ids, positive_context = extract_words_contexts(sample, R)\n",
    "        token_ids.append(sample_token_ids)\n",
    "        positive_contexts.append(positive_context)\n",
    "    return token_ids, positive_contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Apply the function to your initial document_train_set and document_valid_set, and get the corresponding flattened lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 2\n",
    "token_ids, positive_contexts = flatten_dataset_to_list(document_train_set, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Embed these lists in two valid PyTorch Dataset, like in HW 1, call them train_set and valid_set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, document_set, R):\n",
    "        self.document_set = document_set\n",
    "        token_ids, positive_contexts = flatten_dataset_to_list(document_set, R)\n",
    "        self.token_ids = torch.tensor(token_ids)\n",
    "        self.positive_contexts = torch.tensor(positive_contexts)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"word_id\" : self.token_ids[idx], \n",
    "            \"positive_context_ids\" : self.positive_contexts[idx],\n",
    "            # \"label\" : torch.tensor(self.document_set[idx][\"label\"])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CustomDataset(document_train_set, R)\n",
    "valid_set = CustomDataset(document_valid_set, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 4000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_set), len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "try :\n",
    "    valid_set[951], train_set[1347:-2000]\n",
    "except :\n",
    "    print(\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Write a collate_fn function that adds the negative context to the batch. It should be parametrized by the scaling factor K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, R, K, VOCSIZE):\n",
    "    ''' batch is a list of dictionary with keys \"word_id\", \"positive_context_ids\" and \"label\" which contain tensors\n",
    "    What we want is that the output becomes a dictionary with keys :\n",
    "    - \"word_id\", which contains the all the token_ids for every review in the batch. It should be a tensor of shape (batch_size, n_tokens=256)\n",
    "    - \"positive_context_ids\", which contains the positive context of all tokens for every review in the batch. \n",
    "      It should be a tensor of shape (batch_size, n_tokens, 2R)\n",
    "    - \"negative_context_ids\", same thing for negative context. It should be a tensor of shape (batch_size, n_tokens, 2RK)\n",
    "\n",
    "    '''\n",
    "    batch_size = len(batch)\n",
    "    n_tokens = len(batch[0][\"word_id\"])\n",
    "    result = dict()\n",
    "    result[\"word_id\"] = torch.stack([review[\"word_id\"] for review in batch])\n",
    "    result[\"positive_context_ids\"] = torch.stack([review[\"positive_context_ids\"] for review in batch])\n",
    "    # sample 2RK tokens from the vocabulary for each token in each review in the batch -> reshape it -> convert to a tensor\n",
    "    result[\"negative_context_ids\"] = torch.tensor(\n",
    "        np.random.choice(np.arange(VOCSIZE), 2*R*K*n_tokens*batch_size, replace=True)\\\n",
    "            .reshape(batch_size, n_tokens, 2*R*K)\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Wraps everything in a DataLoader, like in HW 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "R = 2\n",
    "K = 2\n",
    "collate_fn_with_params = functools.partial(collate_fn, R=R, K=K, VOCSIZE=VOCSIZE)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_set, batch_size=batch_size, collate_fn=collate_fn_with_params\n",
    ")   \n",
    "valid_dataloader = DataLoader(\n",
    "    valid_set, batch_size=batch_size, collate_fn=collate_fn_with_params\n",
    ")\n",
    "n_valid = len(valid_set)\n",
    "n_train = len(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Make 2 or 3 three iterations in the DataLoader and print R, K and the shapes of all the tensors in the batches (let the output be visible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R = 2\n",
      "K = 2\n",
      "batch 0 :\n",
      "dict_keys(['word_id', 'positive_context_ids', 'negative_context_ids'])\n",
      "'word_id' shape : torch.Size([32, 256])\n",
      "'positive_context_ids' shape : torch.Size([32, 256, 4])\n",
      "'negative_context_ids' shape : torch.Size([32, 256, 8])\n",
      "--------------------------------------------------\n",
      "batch 1 :\n",
      "dict_keys(['word_id', 'positive_context_ids', 'negative_context_ids'])\n",
      "'word_id' shape : torch.Size([32, 256])\n",
      "'positive_context_ids' shape : torch.Size([32, 256, 4])\n",
      "'negative_context_ids' shape : torch.Size([32, 256, 8])\n",
      "--------------------------------------------------\n",
      "batch 2 :\n",
      "dict_keys(['word_id', 'positive_context_ids', 'negative_context_ids'])\n",
      "'word_id' shape : torch.Size([32, 256])\n",
      "'positive_context_ids' shape : torch.Size([32, 256, 4])\n",
      "'negative_context_ids' shape : torch.Size([32, 256, 8])\n",
      "--------------------------------------------------\n",
      "batch 3 :\n",
      "dict_keys(['word_id', 'positive_context_ids', 'negative_context_ids'])\n",
      "'word_id' shape : torch.Size([32, 256])\n",
      "'positive_context_ids' shape : torch.Size([32, 256, 4])\n",
      "'negative_context_ids' shape : torch.Size([32, 256, 8])\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"R =\", R)\n",
    "print(\"K =\", K)\n",
    "\n",
    "for i, batch in enumerate(train_dataloader):\n",
    "    print(f\"batch {i} :\")\n",
    "    print(batch.keys())\n",
    "    for key, value in batch.items():\n",
    "        print(f\"'{key}' shape :\", value.shape)\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Write a model named Word2Vec which is a valid torch.nn.Module (i.e.,\n",
    "write a class that inherits from the torch.nn.Module), and implement the\n",
    "Word2Vec model. It should be parametrized by the vocabulary size and\n",
    "the embeddings dimension. Use the module torch.nn.Embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class Word2Vec(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, VOCSIZE, emb_dim, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.VOCSIZE = VOCSIZE\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        # Layers\n",
    "        self.emb = torch.nn.Embedding(self.VOCSIZE, self.emb_dim, padding_idx=0)\n",
    "    \n",
    "    def similarity(self, word_emb, context_emb) -> Any:\n",
    "        '''Takes the word embeddings, the context_embeddings and compute the dot product between each word embedding and its context embeddings\n",
    "        word_emb : (B,L,E) = (batch_size, max_length, embedding_dim)\n",
    "        context_emb : (B,L,C,E)\n",
    "        output : (B,L,C)\n",
    "        '''\n",
    "        word_emb_expanded = word_emb.unsqueeze(2) # (B,L,1,E)\n",
    "        context_emb_transposed = context_emb.transpose(-1,-2) #(B,L,E,C)\n",
    "        return torch.matmul(word_emb_expanded, context_emb_transposed).squeeze(2) # Matmul -> (B,L,1,C) -> squeezed into (B,L,C)\n",
    "    \n",
    "    def __call__(self, input):\n",
    "        embeddings = dict()\n",
    "        for k,v in input.items():\n",
    "            embeddings[k] = self.emb(v)\n",
    "        positive_similarity = self.similarity(embeddings[\"word_id\"], embeddings[\"positive_context_ids\"])\n",
    "        negative_similarity = self.similarity(embeddings[\"word_id\"], embeddings[\"negative_context_ids\"])\n",
    "        return {\"positive_similarity\":positive_similarity, \"negative_similarity\":negative_similarity}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick sanity check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIM = 50\n",
    "VOCSIZE = tokenizer.vocab_size\n",
    "model = Word2Vec(VOCSIZE, EMB_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['positive_similarity', 'negative_similarity'])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(batch)\n",
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 256, 4]), torch.Size([32, 256, 8]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[\"positive_similarity\"].shape, out[\"negative_similarity\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Train the model. The training should be parametrized by the batch size\n",
    "B, and the number of epochs E."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we denote $y = \\mathbb{1}_{c \\in \\mathcal{C}^+}$, then our loss can be seen as a binary cross-entropy loss :\n",
    "$$ \\frac{1}{n} \\sum_{i=1}^n - [y_i \\log(x_i) + (1-y_i) \\log(1-x_i)]$$ \n",
    "where $x_i$ is $\\sigma(w_i \\cdot c_i)$, (and reduce = 'mean'). <br>\n",
    "Therefore we can use the BCE with logit loss :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.BCELoss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def __call__(self, positive_similarity, negative_similarity):\n",
    "        '''computes the loss\n",
    "        '''\n",
    "        # Positive context\n",
    "        y_positive = torch.ones_like(positive_similarity, dtype=torch.float32)\n",
    "        loss = self.BCELoss(positive_similarity, y_positive)\n",
    "\n",
    "        # Negative context\n",
    "        y_negative = torch.zeros_like(negative_similarity, dtype=torch.float32)\n",
    "        loss += self.BCELoss(negative_similarity, y_negative)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyLoss = CustomLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.1157, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyLoss(**out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Word2Vec(\n",
       "  (emb): Embedding(30522, 50, padding_idx=0)\n",
       ")"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 53.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4.811533749103546, 0.5336046405136585)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validation(model, valid_dataloader, loss_fn):\n",
    "    model.eval()\n",
    "    loss_total = 0.\n",
    "    acc = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_dataloader):\n",
    "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "            output = model(batch)\n",
    "            loss = loss_fn(**output)\n",
    "            loss_total += loss.detach().cpu().item()\n",
    "            total_predictions = output[\"positive_similarity\"].shape.numel() + output[\"negative_similarity\"].shape.numel()\n",
    "            acc += ((torch.sum(output[\"positive_similarity\"]>0)+torch.sum(output[\"negative_similarity\"]<=0))/total_predictions).cpu().item()\n",
    "    return loss_total / len(valid_dataloader), acc / len(valid_dataloader)\n",
    "\n",
    "validation(model, valid_dataloader, MyLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, lr, E, B, loss_fn, train_dataloader, valid_dataloader):\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Performance metric tracking\n",
    "    list_val_acc = []\n",
    "    list_train_acc = []\n",
    "    list_train_loss = []\n",
    "    list_val_loss = []\n",
    "\n",
    "    \n",
    "    for e in range(E):\n",
    "        # ========== Training ==========\n",
    "        model.train()\n",
    "        train_loss = 0.\n",
    "        acc = 0.\n",
    "        for batch in tqdm(train_dataloader):\n",
    "            batch = {k:v.to(DEVICE) for k,v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch)\n",
    "            loss = loss_fn(**output)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.detach().cpu().item()\n",
    "            total_predictions = output[\"positive_similarity\"].shape.numel() + output[\"negative_similarity\"].shape.numel()\n",
    "            acc += ((torch.sum(output[\"positive_similarity\"]>0)+torch.sum(output[\"negative_similarity\"]<=0))/total_predictions).cpu().item()\n",
    "        list_train_loss.append(train_loss / len(train_dataloader))\n",
    "        list_train_acc.append(100 * acc / len(train_dataloader))\n",
    "\n",
    "        # ========== Validation ==========\n",
    "        l, a = validation(model, valid_dataloader, loss_fn)\n",
    "        list_val_loss.append(l)\n",
    "        list_val_acc.append(a * 100)\n",
    "        print(\n",
    "            e,\n",
    "            \"\\n\\t - Train loss: {:.4f}\".format(list_train_loss[-1]),\n",
    "            \"Train acc: {:.4f}\".format(list_train_acc[-1]),\n",
    "            \"Val loss: {:.4f}\".format(l),\n",
    "            \"Val acc:{:.4f}\".format(a * 100),\n",
    "        )\n",
    "    return list_train_loss, list_train_acc, list_val_loss, list_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Word2Vec(\n",
       "  (emb): Embedding(30522, 50, padding_idx=0)\n",
       ")"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:02<00:00, 44.09it/s] \n",
      "100%|██████████| 32/32 [00:00<00:00, 272.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      "\t - Train loss: 4.6534 Train acc: 53.6473 Val loss: 4.5166 Val acc:53.8154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:01<00:00, 103.02it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 295.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      "\t - Train loss: 4.3731 Train acc: 54.0817 Val loss: 4.2585 Val acc:54.2727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:01<00:00, 111.95it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 403.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 \n",
      "\t - Train loss: 4.1268 Train acc: 54.7030 Val loss: 4.0338 Val acc:54.9799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:00<00:00, 128.02it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 223.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 \n",
      "\t - Train loss: 3.9036 Train acc: 55.4203 Val loss: 3.8208 Val acc:55.6834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:02<00:00, 48.73it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 186.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 \n",
      "\t - Train loss: 3.6970 Train acc: 56.0532 Val loss: 3.6315 Val acc:56.2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:02<00:00, 41.71it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 186.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 \n",
      "\t - Train loss: 3.5044 Train acc: 56.8146 Val loss: 3.4546 Val acc:57.0144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:03<00:00, 37.37it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 130.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 \n",
      "\t - Train loss: 3.3280 Train acc: 57.4771 Val loss: 3.2856 Val acc:57.7198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:03<00:00, 35.81it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 124.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 \n",
      "\t - Train loss: 3.1624 Train acc: 58.3108 Val loss: 3.1285 Val acc:58.5523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:03<00:00, 36.02it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 123.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 \n",
      "\t - Train loss: 3.0049 Train acc: 59.1914 Val loss: 2.9798 Val acc:59.3989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:03<00:00, 35.82it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 109.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 \n",
      "\t - Train loss: 2.8568 Train acc: 59.9872 Val loss: 2.8461 Val acc:60.1314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([4.653419181823731,\n",
       "  4.373093809127807,\n",
       "  4.1267561454772945,\n",
       "  3.9036430168151854,\n",
       "  3.6969664497375487,\n",
       "  3.504398693084717,\n",
       "  3.327981958389282,\n",
       "  3.162409549713135,\n",
       "  3.004857526779175,\n",
       "  2.8567676315307615],\n",
       " [53.64733266830444,\n",
       "  54.081658935546876,\n",
       "  54.70304570198059,\n",
       "  55.420314407348634,\n",
       "  56.05323257446289,\n",
       "  56.81460165977478,\n",
       "  57.477077102661134,\n",
       "  58.31084189414978,\n",
       "  59.19135947227478,\n",
       "  59.98721714019776],\n",
       " [4.516626849770546,\n",
       "  4.258542448282242,\n",
       "  4.033826999366283,\n",
       "  3.820789195597172,\n",
       "  3.6315032243728638,\n",
       "  3.4545966908335686,\n",
       "  3.2856349423527718,\n",
       "  3.128538556396961,\n",
       "  2.9797911643981934,\n",
       "  2.8460602909326553],\n",
       " [53.815398551523685,\n",
       "  54.27271742373705,\n",
       "  54.979898408055305,\n",
       "  55.68336043506861,\n",
       "  56.290532648563385,\n",
       "  57.01443590223789,\n",
       "  57.71983675658703,\n",
       "  58.55233073234558,\n",
       "  59.39887557178736,\n",
       "  60.131360962986946])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "lr = 5e-4\n",
    "training(model, lr=lr, E=EPOCHS, B=32, loss_fn=MyLoss, train_dataloader=train_dataloader, valid_dataloader=valid_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
