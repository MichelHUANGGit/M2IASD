{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The first cells of the notebook are the same as in the TP on text convolution. Apply the same preprocessing to get a dataset (with the same tokenizer) with a train and a validation split, with two columns review_ids (list of int) and label (int)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER** : Copying what we did in the TP on text convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\huang\\Desktop\\M2IASD\\NLP\\NLP\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from tabulate import tabulate\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version de pytorch :  2.2.0+cu121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Version de pytorch : \", torch.__version__)\n",
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['review', 'sentiment'],\n",
      "    num_rows: 50000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"scikit-learn/imdb\", split=\"train\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of the tokenizer: <class 'collections.OrderedDict'>\n",
      "Length of the vocabulary: 30522\n",
      "OrderedDict({'[PAD]': 0, '[unused0]': 1, '[unused1\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of the tokenizer:\", type(tokenizer.vocab))\n",
    "VOCSIZE = len(tokenizer.vocab)\n",
    "print(\"Length of the vocabulary:\", VOCSIZE)\n",
    "print(str(tokenizer.vocab)[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_fn(x, tokenizer):\n",
    "    x[\"review_ids\"] = tokenizer(\n",
    "        x[\"review\"],\n",
    "        add_special_tokens=False,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=False,\n",
    "    )[\"input_ids\"]\n",
    "    x[\"label\"] = 0 if x[\"sentiment\"] == \"negative\" else 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:14<00:00, 352.39 examples/s]\n"
     ]
    }
   ],
   "source": [
    "n_samples = 5000  # the number of training example\n",
    "\n",
    "# We first shuffle the data !\n",
    "dataset = dataset.shuffle(seed=0)\n",
    "\n",
    "# Select 5000 samples\n",
    "sampled_dataset = dataset.select(range(n_samples))\n",
    "\n",
    "# Tokenize the dataset\n",
    "sampled_dataset = sampled_dataset.map(\n",
    "    preprocessing_fn, fn_kwargs={\"tokenizer\" : tokenizer}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove useless columns\n",
    "sampled_dataset = sampled_dataset.select_columns(['review_ids','label'])\n",
    "\n",
    "# Split the train and validation\n",
    "splitted_dataset = sampled_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "document_train_set = splitted_dataset['train']\n",
    "document_valid_set = splitted_dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write a function extract_words_contexts. It should retrieve all pairs of valid $(w, C^+)$ from a list of ids representing a text document. It takes the radius $R$ as an argument. Its output is therefore two lists :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to make sure that every C has the same size, we add padding at the beginning and the end of the sentence. For example the first word of the sentence, will have R paddings corresponding to the R tokens that should be before. We can also use the token itself, so that it has a high dot product with itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words_contexts(sample, R):\n",
    "    token_ids = sample[\"review_ids\"]\n",
    "    n_tokens = len(token_ids)\n",
    "    positive_context = []\n",
    "    token_ids_with_padding = [0]*R + token_ids + [0]*R\n",
    "    for i in range(n_tokens) :\n",
    "        # if out of bounds\n",
    "        if i<R or i>=n_tokens-R :\n",
    "            positive_context.append([token_ids_with_padding[i+r] for r in range(R)] + [token_ids_with_padding[i+R+r] for r in range(1,R+1, 1)])\n",
    "        else :\n",
    "            positive_context.append([token_ids[i+r] for r in range(-R, 0, 1)] + [token_ids[i+r] for r in range(1, R+1, 1)])\n",
    "    return token_ids, positive_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "toto, test = extract_words_contexts(document_train_set[2], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 tokens : [2004, 1037, 11798, 5470, 1010]\n",
      "C+ of the first 5 tokens :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 1037, 11798, 5470],\n",
       " [0, 0, 2004, 11798, 5470, 1010],\n",
       " [0, 2004, 1037, 5470, 1010, 1045],\n",
       " [2004, 1037, 11798, 1010, 1045, 2428],\n",
       " [1037, 11798, 5470, 1045, 2428, 2293]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"First 5 tokens :\", toto[:5])\n",
    "print(\"C+ of the first 5 tokens :\")\n",
    "test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write a function flatten_dataset_to_list that applies the function extract_words_contexts on a whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dataset_to_list(dataset, R):\n",
    "    token_ids = []\n",
    "    positive_contexts = []\n",
    "    for sample in dataset:\n",
    "        sample_token_ids, positive_context = extract_words_contexts(sample, R)\n",
    "        token_ids.append(sample_token_ids)\n",
    "        positive_contexts.append(positive_context)\n",
    "    return token_ids, positive_contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Apply the function to your initial document_train_set and document_valid_set, and get the corresponding flattened lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 2\n",
    "token_ids, positive_contexts = flatten_dataset_to_list(document_train_set, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Embed these lists in two valid PyTorch Dataset, like in HW 1, call them train_set and valid_set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, document_set, R):\n",
    "        self.document_set = document_set\n",
    "        token_ids, positive_contexts = flatten_dataset_to_list(document_set, R)\n",
    "        self.token_ids = torch.tensor(token_ids)\n",
    "        self.positive_contexts = torch.tensor(positive_contexts)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # outputs a dictionary of lists\n",
    "        # return self.document_set[idx]\n",
    "        return {\n",
    "            \"word_id\" : self.token_ids[idx], \n",
    "            \"positive_context_ids\" : self.positive_contexts[idx],\n",
    "            \"label\" : torch.tensor(self.document_set[idx][\"label\"])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CustomDataset(document_train_set, R)\n",
    "valid_set = CustomDataset(document_valid_set, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 4000)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_set), len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'word_id': tensor([ 2074,  2234,  2067,  2013,  1996,  2034,  4760,  1997,  3937, 12753,\n",
       "           1016,  1012,  1045,  2001,  2183,  2046,  2009,  3241,  2009,  2052,\n",
       "           2022, 10231,  7685,  2241,  2006, 19236,  4401,  1998,  1045,  2001,\n",
       "          27726,  4527,   999,  2065,  2017,  4669,  1996,  2434,  3937, 12753,\n",
       "           1045,  2228,  2017,  2097,  5959,  1001,  1016,  2074,  2004,  2172,\n",
       "           2065,  2025,  2062,  1012,  2307,  2466,  2008,  2467,  7906,  2017,\n",
       "           6603,  1998,  3241,  1012,  1996,  2189,  2003, 21688,  1010, 16360,\n",
       "           6935,  2075,  1996,  2434,  1005,  1055,  4323,  1012,  2123,  1005,\n",
       "           1056,  2175,  8074,  2914,  2400,  3430,  1010,  2175,  2000,  2156,\n",
       "           2009,  2005, 20195,  1998,  4569,  1012,  2008,  1005,  1055,  2054,\n",
       "           5691,  2024,  2881,  2005,  1011,  1011,  9686, 17695,  2964,  1012,\n",
       "           1045,  2064,  1005,  1056,  2228,  1997,  1037,  2488,  2126,  2000,\n",
       "           4019,  2084,  2000,  4019,  2007, 10666,  2962,  2040,  2003,  2004,\n",
       "           7916,  2004,  2016,  2412,  2001,  1012,  2572,  3241,  2055,  2183,\n",
       "           2000,  2156,  2009,  2153,  2023,  5353,  1012,  2175,  2156,  2009,\n",
       "            999,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0]),\n",
       "  'positive_context_ids': tensor([[   0,    0, 2234, 2067],\n",
       "          [   0, 2074, 2067, 2013],\n",
       "          [2074, 2234, 2013, 1996],\n",
       "          ...,\n",
       "          [   0,    0,    0,    0],\n",
       "          [   0,    0,    0,    0],\n",
       "          [   0,    0,    0,    0]]),\n",
       "  'label': tensor(1)},\n",
       " {'word_id': tensor([[ 2023,  2003,  3243,  4298,  1996,  5409,  2143,  1045,  2031,  2412,\n",
       "            2464,  1012,  1045,  2052,  2228,  2017,  2071,  2131,  2008,  2013,\n",
       "            1996,  2516,  1012,  2036,  1010,  2045,  2003,  1037,  3327,  2293,\n",
       "            3496,  2008,  2071,  2022,  1996,  4326,  3367,  1999,  1996,  2381,\n",
       "            1997,  2143,  1012,  1045,  2064,  1005,  1056,  2130,  3342,  2339,\n",
       "            1045,  2387,  2023,  2143,  2030,  2043,  1012,  2069,  2008,  2003,\n",
       "            2019,  7078,  9202,  3185,  1011, 10523,  3325,  1012,  2006,  1996,\n",
       "            2060,  2192,  1010,  2065,  2017,  2024,  2559,  2005,  1996,  7619,\n",
       "            6881,  4355,  3185,  2000,  5949,  2048,  2847,  1997,  2115,  2051,\n",
       "            1010,  2059,  2011,  2035,  2965,  9278,  2009,  1012,  2204,  6735,\n",
       "            4531,  2009,  2012,  2115,  2334,  3573,  2295,  1012,  1045,  4797,\n",
       "            2023,  3185,  2003,  1999,  1037,  2200,  2898,  1011,  4353,  1012,\n",
       "            1998,  3531,  2079,  2025,  2265,  2023,  2000,  2336,  2011,  2151,\n",
       "            2965,  2004,  2009,  2089, 24136,  2037,  8605,  3085,  9273,  5091,\n",
       "            1012,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0],\n",
       "          [ 2096,  4439,  1999,  1037,  3307,  2000,  1996,  5030,  1997,  2010,\n",
       "           11419,  9306,  1011,  5754,  1010,  4205,  1006,  4388, 11810,  5804,\n",
       "            1007,  2003,  4527,  2011,  2010,  2280,  2082,  8585, 13653,  1006,\n",
       "            6796, 24471,  7033,  1007,  2006,  1996, 19978,  1997,  2010,  2482,\n",
       "            1012,  4205,  2038,  3714,  2125,  2007,  1996,  4297,  2239,  8159,\n",
       "           11638,  1998, 22822,  2239, 13653,  2138,  1997,  9306,  1011,  5754,\n",
       "            1012,  2247,  2037,  2346,  4440,  1010, 13653,  3084,  4569,  1997,\n",
       "            2070,  2417, 18278,  2015,  1999,  1037,  3347,  1998,  2101,  2037,\n",
       "            2482,  2003, 13303,  2011,  1037,  5016,  6071,  4744,  2006,  1996,\n",
       "            2346,  1012,  2044,  2070, 10444,  1010,  2027,  2507,  1037,  6336,\n",
       "            2000,  1996, 27738,  4048,  5484,  4532,  1006, 23551,  8379,  1007,\n",
       "            1998, 10076,  1996,  7146,  2003,  7404,  3550,  2011,  1037, 12459,\n",
       "            6071,  4439,  1996,  6071,  4744,  1012,  1026,  7987,  1013,  1028,\n",
       "            1026,  7987,  1013,  1028,  1999,  8741,  1997,  2383,  2028,  1997,\n",
       "            1996,  2087, 15703,  3494,  1045,  2031,  2412,  2464,  1999,  1037,\n",
       "            5469,  3185,  1010,  1996, 29348, 13653,  1010,  1000,  6071,  2158,\n",
       "            1000,  2003,  1037, 10889,  2204, 11669,  5469,  1011,  4038,  1012,\n",
       "            1996,  2466,  2003,  1037,  3074,  1997, 18856, 17322,  2015,  1010,\n",
       "            2927,  2066,  1000,  6569,  4536,  1000,  2030,  1000, 14216,  1000,\n",
       "            1025,  2059,  2009,  4332,  2000,  2028,  1997,  1996, 14518, 10973,\n",
       "            1011, 12446,  1997,  1000,  1996,  3146,  8859, 10376,  9288,  1000,\n",
       "            1025,  2045,  2003,  1037, 11341,  9792,  1010,  4566,  2007,  1037,\n",
       "            8103,  2005,  1037,  8297,  1012,  2045,  2024, 26316,  5019,  1010,\n",
       "           23551,  8379,  2003,  5186,  7916,  1998,  2023,  2143,  2428, 20432,\n",
       "            2015,  1012,  2026,  3789,  2003,  2698,  1012,  1026,  7987,  1013,\n",
       "            1028,  1026,  7987,  1013,  1028,  2516]]),\n",
       "  'positive_context_ids': tensor([[[   0,    0, 2003, 3243],\n",
       "           [   0, 2023, 3243, 4298],\n",
       "           [2023, 2003, 4298, 1996],\n",
       "           ...,\n",
       "           [   0,    0,    0,    0],\n",
       "           [   0,    0,    0,    0],\n",
       "           [   0,    0,    0,    0]],\n",
       "  \n",
       "          [[   0,    0, 4439, 1999],\n",
       "           [   0, 2096, 1999, 1037],\n",
       "           [2096, 4439, 1037, 3307],\n",
       "           ...,\n",
       "           [1026, 7987, 1028, 2516],\n",
       "           [7987, 1013, 2516,    0],\n",
       "           [1013, 1028,    0,    0]]]),\n",
       "  'label': tensor([0, 1])})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_set[951], train_set[1345:1347]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['word_id', 'positive_context_ids', 'label'])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[:5].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word_id': tensor([[ 2024,  2017,  5220,  ...,  1000,  2190,  1997],\n",
       "         [ 2023,  2003,  1996,  ...,  2123,  1005,  1056],\n",
       "         [ 2004,  1037, 11798,  ...,     0,     0,     0],\n",
       "         [ 2023,  3319,  3397,  ...,  2006,  1037, 13359],\n",
       "         [ 2045,  2024,  2335,  ...,  2066,  1000,  8840]]),\n",
       " 'positive_context_ids': tensor([[[    0,     0,  2017,  5220],\n",
       "          [    0,  2024,  5220,  2007],\n",
       "          [ 2024,  2017,  2007,  4145],\n",
       "          ...,\n",
       "          [ 2061,  2116,  2190,  1997],\n",
       "          [ 2116,  1000,  1997,     0],\n",
       "          [ 1000,  2190,     0,     0]],\n",
       " \n",
       "         [[    0,     0,  2003,  1996],\n",
       "          [    0,  2023,  1996,  2309],\n",
       "          [ 2023,  2003,  2309,  4602],\n",
       "          ...,\n",
       "          [ 1012,  2339,  1005,  1056],\n",
       "          [ 2339,  2123,  1056,     0],\n",
       "          [ 2123,  1005,     0,     0]],\n",
       " \n",
       "         [[    0,     0,  1037, 11798],\n",
       "          [    0,  2004, 11798,  5470],\n",
       "          [ 2004,  1037,  5470,  1010],\n",
       "          ...,\n",
       "          [    0,     0,     0,     0],\n",
       "          [    0,     0,     0,     0],\n",
       "          [    0,     0,     0,     0]],\n",
       " \n",
       "         [[    0,     0,  3319,  3397],\n",
       "          [    0,  2023,  3397, 27594],\n",
       "          [ 2023,  3319, 27594,  2545],\n",
       "          ...,\n",
       "          [ 2040,  8509,  1037, 13359],\n",
       "          [ 8509,  2006, 13359,     0],\n",
       "          [ 2006,  1037,     0,     0]],\n",
       " \n",
       "         [[    0,     0,  2024,  2335],\n",
       "          [    0,  2045,  2335,  2043],\n",
       "          [ 2045,  2024,  2043,  1010],\n",
       "          ...,\n",
       "          [ 1005,  1055,  1000,  8840],\n",
       "          [ 1055,  2066,  8840,     0],\n",
       "          [ 2066,  1000,     0,     0]]]),\n",
       " 'label': tensor([0, 1, 1, 1, 0])}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Write a collate_fn function that adds the negative context to the batch. It should be parametrized by the scaling factor K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, R, K, VOCSIZE):\n",
    "    ''' batch is a list of dictionary with keys \"word_id\", \"positive_context_ids\" and \"label\" which contain tensors\n",
    "    What we want is that the output becomes a dictionary with keys :\n",
    "    - \"word_id\", which contains the all the token_ids for every review in the batch. It should be a tensor of shape (batch_size, n_tokens=256)\n",
    "    - \"positive_context_ids\", which contains the positive context of all tokens for every review in the batch. \n",
    "      It should be a tensor of shape (batch_size, n_tokens, 2R)\n",
    "    - \"negative_context_ids\", same thing for negative context. It should be a tensor of shape (batch_size, n_tokens, 2RK)\n",
    "    - \"label\"\n",
    "\n",
    "    '''\n",
    "    batch_size = len(batch)\n",
    "    n_tokens = len(batch[0][\"word_id\"])\n",
    "    result = dict()\n",
    "    result[\"word_id\"] = torch.stack([review[\"word_id\"] for review in batch])\n",
    "    result[\"positive_context_ids\"] = torch.stack([review[\"positive_context_ids\"] for review in batch])\n",
    "    result[\"labels\"] = torch.stack([review[\"label\"] for review in batch])\n",
    "    # sample 2RK tokens from the vocabulary for each token for each review in the batch -> reshape it -> convert to a tensor\n",
    "    result[\"negative_context_ids\"] = torch.tensor(\n",
    "        np.random.choice(np.arange(VOCSIZE), 2*R*K*n_tokens*batch_size, replace=True)\\\n",
    "            .reshape(batch_size, n_tokens, 2*R*K)\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Wraps everything in a DataLoader, like in HW 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "R = 2\n",
    "K = 2\n",
    "collate_fn_with_params = functools.partial(collate_fn, R=R, K=K, VOCSIZE=VOCSIZE)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_set, batch_size=batch_size, collate_fn=collate_fn_with_params\n",
    ")   \n",
    "valid_dataloader = DataLoader(\n",
    "    valid_set, batch_size=batch_size, collate_fn=collate_fn_with_params\n",
    ")\n",
    "n_valid = len(valid_set)\n",
    "n_train = len(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Make 2 or 3 three iterations in the DataLoader and print R, K and the shapes of all the tensors in the batches (let the output be visible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R = 2\n",
      "K = 2\n",
      "batch 0 :\n",
      "dict_keys(['word_id', 'positive_context_ids', 'labels', 'negative_context_ids'])\n",
      "'word_id' shape : torch.Size([32, 256])\n",
      "'positive_context_ids' shape : torch.Size([32, 256, 4])\n",
      "'labels' shape : torch.Size([32])\n",
      "'negative_context_ids' shape : torch.Size([32, 256, 8])\n",
      "--------------------------------------------------\n",
      "batch 1 :\n",
      "dict_keys(['word_id', 'positive_context_ids', 'labels', 'negative_context_ids'])\n",
      "'word_id' shape : torch.Size([32, 256])\n",
      "'positive_context_ids' shape : torch.Size([32, 256, 4])\n",
      "'labels' shape : torch.Size([32])\n",
      "'negative_context_ids' shape : torch.Size([32, 256, 8])\n",
      "--------------------------------------------------\n",
      "batch 2 :\n",
      "dict_keys(['word_id', 'positive_context_ids', 'labels', 'negative_context_ids'])\n",
      "'word_id' shape : torch.Size([32, 256])\n",
      "'positive_context_ids' shape : torch.Size([32, 256, 4])\n",
      "'labels' shape : torch.Size([32])\n",
      "'negative_context_ids' shape : torch.Size([32, 256, 8])\n",
      "--------------------------------------------------\n",
      "batch 3 :\n",
      "dict_keys(['word_id', 'positive_context_ids', 'labels', 'negative_context_ids'])\n",
      "'word_id' shape : torch.Size([32, 256])\n",
      "'positive_context_ids' shape : torch.Size([32, 256, 4])\n",
      "'labels' shape : torch.Size([32])\n",
      "'negative_context_ids' shape : torch.Size([32, 256, 8])\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"R =\", R)\n",
    "print(\"K =\", K)\n",
    "\n",
    "for i, batch in enumerate(train_dataloader):\n",
    "    print(f\"batch {i} :\")\n",
    "    print(batch.keys())\n",
    "    for key, value in batch.items():\n",
    "        print(f\"'{key}' shape :\", value.shape)\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
