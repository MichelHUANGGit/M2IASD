{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The first cells of the notebook are the same as in the TP on text convolution. Apply the same preprocessing to get a dataset (with the same tokenizer) with a train and a validation split, with two columns review_ids (list of int) and label (int)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER** : Copying what we did in the TP on text convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\huang\\Desktop\\M2IASD\\NLP\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tabulate import tabulate\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import functools\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version de pytorch :  2.3.0+cu121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Version de pytorch : \", torch.__version__)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['review', 'sentiment'],\n",
      "    num_rows: 50000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"scikit-learn/imdb\", split=\"train\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of the tokenizer: <class 'collections.OrderedDict'>\n",
      "Length of the vocabulary: 30522\n",
      "OrderedDict({'[PAD]': 0, '[unused0]': 1, '[unused1\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of the tokenizer:\", type(tokenizer.vocab))\n",
    "VOCSIZE = len(tokenizer.vocab)\n",
    "print(\"Length of the vocabulary:\", VOCSIZE)\n",
    "print(str(tokenizer.vocab)[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_fn(x, tokenizer):\n",
    "    x[\"review_ids\"] = tokenizer(\n",
    "        x[\"review\"],\n",
    "        add_special_tokens=False,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=False,\n",
    "    )[\"input_ids\"]\n",
    "    x[\"label\"] = 0 if x[\"sentiment\"] == \"negative\" else 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 5000  # the number of training example\n",
    "\n",
    "# We first shuffle the data !\n",
    "dataset = dataset.shuffle(seed=0)\n",
    "\n",
    "# Select 5000 samples\n",
    "sampled_dataset = dataset.select(range(n_samples))\n",
    "\n",
    "# Tokenize the dataset\n",
    "sampled_dataset = sampled_dataset.map(\n",
    "    preprocessing_fn, fn_kwargs={\"tokenizer\" : tokenizer}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove useless columns\n",
    "sampled_dataset = sampled_dataset.select_columns(['review_ids','label'])\n",
    "\n",
    "# Split the train and validation\n",
    "splitted_dataset = sampled_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "document_train_set = splitted_dataset['train']\n",
    "document_valid_set = splitted_dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write a function extract_words_contexts. It should retrieve all pairs of valid $(w, C^+)$ from a list of ids representing a text document. It takes the radius $R$ as an argument. Its output is therefore two lists :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to make sure that every C has the same size, we add padding at the beginning and the end of the sentence. For example the first word of the sentence, will have R paddings corresponding to the R tokens that should be before. We can also use the token itself, so that it has a high dot product with itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words_contexts(sample, R):\n",
    "    token_ids = sample[\"review_ids\"]\n",
    "    n_tokens = len(token_ids)\n",
    "    positive_context = []\n",
    "    token_ids_with_padding = [0]*R + token_ids + [0]*R\n",
    "    for i in range(n_tokens) :\n",
    "        # if out of bounds\n",
    "        if i<R or i>=n_tokens-R :\n",
    "            positive_context.append([token_ids_with_padding[i+r] for r in range(R)] + [token_ids_with_padding[i+R+r] for r in range(1,R+1, 1)])\n",
    "        else :\n",
    "            positive_context.append([token_ids[i+r] for r in range(-R, 0, 1)] + [token_ids[i+r] for r in range(1, R+1, 1)])\n",
    "    return token_ids, positive_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "toto, test = extract_words_contexts(document_train_set[2], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 tokens : [2065, 15555, 22308, 2020, 2025]\n",
      "C+ of the first 5 tokens :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 15555, 22308, 2020],\n",
       " [0, 0, 2065, 22308, 2020, 2025],\n",
       " [0, 2065, 15555, 2020, 2025, 2525],\n",
       " [2065, 15555, 22308, 2025, 2525, 3459],\n",
       " [15555, 22308, 2020, 2525, 3459, 2004]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"First 5 tokens :\", toto[:5])\n",
    "print(\"C+ of the first 5 tokens :\")\n",
    "test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write a function flatten_dataset_to_list that applies the function extract_words_contexts on a whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dataset_to_list(dataset, R):\n",
    "    '''takes a dataset and returns the token_ids and positive context'''\n",
    "    token_ids = []\n",
    "    positive_contexts = []\n",
    "    for sample in dataset:\n",
    "        sample_token_ids, positive_context = extract_words_contexts(sample, R)\n",
    "        token_ids.append(sample_token_ids)\n",
    "        positive_contexts.append(positive_context)\n",
    "    return token_ids, positive_contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Apply the function to your initial document_train_set and document_valid_set, and get the corresponding flattened lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 2\n",
    "token_ids, positive_contexts = flatten_dataset_to_list(document_train_set, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Embed these lists in two valid PyTorch Dataset, like in HW 1, call them train_set and valid_set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, document_set, R):\n",
    "        self.document_set = document_set\n",
    "        token_ids, positive_contexts = flatten_dataset_to_list(document_set, R)\n",
    "        self.token_ids = torch.tensor(token_ids)\n",
    "        self.positive_contexts = torch.tensor(positive_contexts)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"word_id\" : self.token_ids[idx], \n",
    "            \"positive_context_ids\" : self.positive_contexts[idx],\n",
    "            # \"label\" : torch.tensor(self.document_set[idx][\"label\"])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CustomDataset(document_train_set, R)\n",
    "valid_set = CustomDataset(document_valid_set, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 4000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_set), len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "try :\n",
    "    valid_set[951], train_set[1347:-2000]\n",
    "except :\n",
    "    print(\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Write a collate_fn function that adds the negative context to the batch. It should be parametrized by the scaling factor K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, R, K, VOCSIZE):\n",
    "    ''' batch is a list of dictionary with keys \"word_id\", \"positive_context_ids\" and \"label\" which contain tensors\n",
    "    What we want is that the output becomes a dictionary with keys :\n",
    "    - \"word_id\", which contains the all the token_ids for every review in the batch. It should be a tensor of shape (batch_size, n_tokens=256)\n",
    "    - \"positive_context_ids\", which contains the positive context of all tokens for every review in the batch. \n",
    "      It should be a tensor of shape (batch_size, n_tokens, 2R)\n",
    "    - \"negative_context_ids\", same thing for negative context. It should be a tensor of shape (batch_size, n_tokens, 2RK)\n",
    "    '''\n",
    "    batch_size = len(batch)\n",
    "    n_tokens = len(batch[0][\"word_id\"])\n",
    "    result = dict()\n",
    "    result[\"word_id\"] = torch.stack([review[\"word_id\"] for review in batch])\n",
    "    result[\"positive_context_ids\"] = torch.stack([review[\"positive_context_ids\"] for review in batch])\n",
    "    # sample 2RK tokens from the vocabulary for each token in each review in the batch -> reshape it -> convert to a tensor\n",
    "    result[\"negative_context_ids\"] = torch.tensor(\n",
    "        np.random.choice(np.arange(VOCSIZE), 2*R*K*n_tokens*batch_size, replace=True)\\\n",
    "            .reshape(batch_size, n_tokens, 2*R*K)\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Wraps everything in a DataLoader, like in HW 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "R = 2\n",
    "K = 2\n",
    "collate_fn_with_params = functools.partial(collate_fn, R=R, K=K, VOCSIZE=VOCSIZE)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_set, batch_size=batch_size, collate_fn=collate_fn_with_params\n",
    ")   \n",
    "valid_dataloader = DataLoader(\n",
    "    valid_set, batch_size=batch_size, collate_fn=collate_fn_with_params\n",
    ")\n",
    "n_valid = len(valid_set)\n",
    "n_train = len(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Make 2 or 3 three iterations in the DataLoader and print R, K and the shapes of all the tensors in the batches (let the output be visible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R = 2\n",
      "K = 2\n",
      "batch 0 :\n",
      "dict_keys(['word_id', 'positive_context_ids', 'negative_context_ids'])\n",
      "'word_id' shape : torch.Size([32, 256])\n",
      "'positive_context_ids' shape : torch.Size([32, 256, 4])\n",
      "'negative_context_ids' shape : torch.Size([32, 256, 8])\n",
      "--------------------------------------------------\n",
      "batch 1 :\n",
      "dict_keys(['word_id', 'positive_context_ids', 'negative_context_ids'])\n",
      "'word_id' shape : torch.Size([32, 256])\n",
      "'positive_context_ids' shape : torch.Size([32, 256, 4])\n",
      "'negative_context_ids' shape : torch.Size([32, 256, 8])\n",
      "--------------------------------------------------\n",
      "batch 2 :\n",
      "dict_keys(['word_id', 'positive_context_ids', 'negative_context_ids'])\n",
      "'word_id' shape : torch.Size([32, 256])\n",
      "'positive_context_ids' shape : torch.Size([32, 256, 4])\n",
      "'negative_context_ids' shape : torch.Size([32, 256, 8])\n",
      "--------------------------------------------------\n",
      "batch 3 :\n",
      "dict_keys(['word_id', 'positive_context_ids', 'negative_context_ids'])\n",
      "'word_id' shape : torch.Size([32, 256])\n",
      "'positive_context_ids' shape : torch.Size([32, 256, 4])\n",
      "'negative_context_ids' shape : torch.Size([32, 256, 8])\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"R =\", R)\n",
    "print(\"K =\", K)\n",
    "\n",
    "for i, batch in enumerate(train_dataloader):\n",
    "    print(f\"batch {i} :\")\n",
    "    print(batch.keys())\n",
    "    for key, value in batch.items():\n",
    "        print(f\"'{key}' shape :\", value.shape)\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Write a model named Word2Vec which is a valid torch.nn.Module (i.e.,\n",
    "write a class that inherits from the torch.nn.Module), and implement the\n",
    "Word2Vec model. It should be parametrized by the vocabulary size and\n",
    "the embeddings dimension. Use the module torch.nn.Embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to that model should directly output the similarity, not just the embeddings :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, VOCSIZE, emb_dim, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.VOCSIZE = VOCSIZE\n",
    "        self.emb_dim = emb_dim\n",
    "        self.emb = torch.nn.Embedding(self.VOCSIZE, self.emb_dim, padding_idx=0)\n",
    "    \n",
    "    def similarity(self, word_emb, context_emb) -> Any:\n",
    "        '''Takes the word embeddings, the context_embeddings and compute the dot product between each word embedding and its context embeddings\n",
    "        word_emb : (B,L,E) = (batch_size, max_length, embedding_dim)\n",
    "        context_emb : (B,L,C,E)\n",
    "        output : (B,L,C)\n",
    "        '''\n",
    "        word_emb_expanded = word_emb.unsqueeze(2) # (B,L,E) -> (B,L,1,E)\n",
    "        context_emb_transposed = context_emb.transpose(-1,-2) #(B,L,C,E) -> (B,L,E,C)\n",
    "        return torch.matmul(word_emb_expanded, context_emb_transposed).squeeze(2) # (B,L,1,E) @ (B,L,E,C) -> (B,L,1,C) -> squeezed into (B,L,C)\n",
    "    \n",
    "    def __call__(self, input:dict):\n",
    "        embeddings = dict()\n",
    "        for k,v in input.items():\n",
    "            embeddings[k] = self.emb(v)\n",
    "        positive_similarity = self.similarity(embeddings[\"word_id\"], embeddings[\"positive_context_ids\"])\n",
    "        negative_similarity = self.similarity(embeddings[\"word_id\"], embeddings[\"negative_context_ids\"])\n",
    "        return {\"positive_similarity\":positive_similarity, \"negative_similarity\":negative_similarity}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick sanity check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIM = 50\n",
    "VOCSIZE = tokenizer.vocab_size\n",
    "model = Word2Vec(VOCSIZE, EMB_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['positive_similarity', 'negative_similarity'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(batch)\n",
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 256, 4]), torch.Size([32, 256, 8]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[\"positive_similarity\"].shape, out[\"negative_similarity\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. Train the model. The training should be parametrized by the batch size B, and the number of epochs E.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we denote $y = \\mathbb{1}_{c \\in \\mathcal{C}^+}$, then our loss can be seen as a binary cross-entropy loss :\n",
    "$$ \\frac{1}{n} \\sum_{i=1}^n - [y_i \\log(x_i) + (1-y_i) \\log(1-x_i)]$$ \n",
    "where $x_i$ is $\\sigma(w_i \\cdot c_i)$, (and reduce = 'mean'). <br>\n",
    "Therefore we can use the torch module BCEWithLogitLoss :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.BCELoss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def __call__(self, positive_similarity, negative_similarity):\n",
    "        '''computes the loss'''\n",
    "        # Positive context\n",
    "        y_positive = torch.ones_like(positive_similarity, dtype=torch.float32)\n",
    "        loss = self.BCELoss(positive_similarity, y_positive)\n",
    "\n",
    "        # Negative context\n",
    "        y_negative = torch.zeros_like(negative_similarity, dtype=torch.float32)\n",
    "        loss += self.BCELoss(negative_similarity, y_negative)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyLoss = CustomLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.7726, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyLoss(**out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Word2Vec(\n",
       "  (emb): Embedding(30522, 50, padding_idx=0)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 62.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4.9726331532001495, 0.5337619930505753)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validation(model, valid_dataloader, loss_fn):\n",
    "    model.eval()\n",
    "    loss_total = 0.\n",
    "    acc = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_dataloader):\n",
    "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "            output = model(batch)\n",
    "            loss = loss_fn(**output)\n",
    "            loss_total += loss.detach().cpu().item()\n",
    "            total_predictions = output[\"positive_similarity\"].shape.numel() + output[\"negative_similarity\"].shape.numel()\n",
    "            acc += ((torch.sum(output[\"positive_similarity\"]>0)+torch.sum(output[\"negative_similarity\"]<=0))/total_predictions).cpu().item()\n",
    "    return loss_total / len(valid_dataloader), acc / len(valid_dataloader)\n",
    "\n",
    "validation(model, valid_dataloader, MyLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, lr, E, B, loss_fn, train_dataloader, valid_dataloader, writer):\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Performance metric tracking\n",
    "    list_val_acc = []\n",
    "    list_train_acc = []\n",
    "    list_train_loss = []\n",
    "    list_val_loss = []\n",
    "\n",
    "    for e in range(E):\n",
    "        # ========== Training ==========\n",
    "        model.train()\n",
    "        train_loss = 0.\n",
    "        acc = 0.\n",
    "        for batch in tqdm(train_dataloader):\n",
    "            batch = {k:v.to(DEVICE) for k,v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch)\n",
    "            loss = loss_fn(**output)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.detach().cpu().item()\n",
    "            total_predictions = output[\"positive_similarity\"].shape.numel() + output[\"negative_similarity\"].shape.numel()\n",
    "            acc += ((torch.sum(output[\"positive_similarity\"]>0)+torch.sum(output[\"negative_similarity\"]<=0))/total_predictions).cpu().item()\n",
    "        list_train_loss.append(train_loss / len(train_dataloader))\n",
    "        list_train_acc.append(100 * acc / len(train_dataloader))\n",
    "\n",
    "        # ========== Validation ==========\n",
    "        l, a = validation(model, valid_dataloader, loss_fn)\n",
    "        list_val_loss.append(l)\n",
    "        list_val_acc.append(a * 100)\n",
    "        # Tensorboard\n",
    "        writer.add_scalar(\"Train loss\", list_train_loss[-1], e)\n",
    "        writer.add_scalar(\"Val loss\", l, e)\n",
    "        writer.add_scalar(\"Train acc\", list_train_acc[-1], e)\n",
    "        writer.add_scalar(\"Val acc\", a, e)\n",
    "        print(\n",
    "            e,\n",
    "            \"\\n\\t - Train loss: {:.4f}\".format(list_train_loss[-1]),\n",
    "            \"Train acc: {:.4f}\".format(list_train_acc[-1]),\n",
    "            \"Val loss: {:.4f}\".format(l),\n",
    "            \"Val acc:{:.4f}\".format(a * 100),\n",
    "        )\n",
    "    return list_train_loss, list_train_acc, list_val_loss, list_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Word2Vec(\n",
       "  (emb): Embedding(30522, 50, padding_idx=0)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:00<00:00, 145.87it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 470.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      "\t - Train loss: 4.7808 Train acc: 53.6133 Val loss: 4.6216 Val acc:53.8432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:00<00:00, 179.55it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 501.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      "\t - Train loss: 4.4525 Train acc: 54.1924 Val loss: 4.3307 Val acc:54.6324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:00<00:00, 178.58it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 479.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 \n",
      "\t - Train loss: 4.1755 Train acc: 54.9755 Val loss: 4.0723 Val acc:55.2553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:00<00:00, 178.84it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 467.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 \n",
      "\t - Train loss: 3.9257 Train acc: 55.5758 Val loss: 3.8388 Val acc:55.7857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:00<00:00, 178.22it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 486.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 \n",
      "\t - Train loss: 3.7024 Train acc: 56.3578 Val loss: 3.6394 Val acc:56.6300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:00<00:00, 178.18it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 493.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 \n",
      "\t - Train loss: 3.5063 Train acc: 57.1039 Val loss: 3.4510 Val acc:57.3222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:00<00:00, 178.96it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 538.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 \n",
      "\t - Train loss: 3.3223 Train acc: 57.8958 Val loss: 3.2763 Val acc:58.1180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:00<00:00, 178.25it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 435.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 \n",
      "\t - Train loss: 3.1515 Train acc: 58.6698 Val loss: 3.1175 Val acc:58.9888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:00<00:00, 179.28it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 532.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 \n",
      "\t - Train loss: 2.9898 Train acc: 59.5837 Val loss: 2.9672 Val acc:59.7988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:00<00:00, 180.12it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 487.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 \n",
      "\t - Train loss: 2.8404 Train acc: 60.4203 Val loss: 2.8236 Val acc:60.6061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "E = 10\n",
    "lr = 5e-4\n",
    "B=32\n",
    "writer = SummaryWriter(\"runs/TestWord2Vec\")\n",
    "list_train_loss, list_train_acc, list_val_loss, list_val_acc = training(\n",
    "    model=model, \n",
    "    lr=lr,\n",
    "    E=E, \n",
    "    B=B, \n",
    "    loss_fn=MyLoss, \n",
    "    train_dataloader=train_dataloader, \n",
    "    valid_dataloader=valid_dataloader,\n",
    "    writer=writer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11. Validates its accuracy on the test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 424.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.8234601840376854, 0.6060091890394688)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation(model, valid_dataloader, MyLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no sign of overfitting yet, the model clearly hasn't finished training yet !\n",
    "Let's take a look at the dot product of a few tokens :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive_similarity': tensor([[[[-5.4205]]]], device='cuda:0'),\n",
       " 'negative_similarity': tensor([[[[-5.4819]]]], device='cuda:0')}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_similarity(model, w, Cplus, Cminus):\n",
    "    token1_id = tokenizer.convert_tokens_to_ids(w)\n",
    "    token2_id = tokenizer.convert_tokens_to_ids(Cplus)\n",
    "    R = len(token2_id) if isinstance(token2_id, list) else 1\n",
    "    token3_id = tokenizer.convert_tokens_to_ids(Cminus)\n",
    "    RK = len(token3_id) if isinstance(token3_id, list) else 1\n",
    "    model_input = {\n",
    "        \"word_id\":torch.tensor([token1_id]).view(1,1,1).cuda(), \n",
    "        \"positive_context_ids\":torch.tensor([token2_id]).view(1,1,R).cuda(),\n",
    "        \"negative_context_ids\":torch.tensor([token3_id]).view(1,1,RK).cuda()\n",
    "    }\n",
    "    # print(model_input)\n",
    "    with torch.no_grad():\n",
    "        return model(model_input)\n",
    "\n",
    "test_similarity(model, \"I\", [\"am\"], \"earth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive_similarity': tensor([[[[8.2870]]]], device='cuda:0'),\n",
       " 'negative_similarity': tensor([[[[4.2378]]]], device='cuda:0')}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_similarity(model, \"good\", \"movie\", \"random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some embeddings seem to be well learned already, some not so much, for example \"I\" and \"earth\" have a positive dot product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12. Write a function save_model that saves the model’s embeddings in a file.** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, root):\n",
    "    embeddings = model.emb.weight\n",
    "    model_filepath = root + f\"model_dim-{model.emb_dim}_radius-{R}_ratio-{K}-batch-{B}-epoch-{E}.ckpt\"\n",
    "    torch.save(embeddings, model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**13. Once you have a working code, you can launch a bigger training, using more documents, if it does not take too much time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the full dataset with 50k reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shuffle(seed=0)\n",
    "full_dataset = dataset.select_columns([\"review\", \"sentiment\"])\n",
    "full_dataset = full_dataset.map(\n",
    "    preprocessing_fn, fn_kwargs={\"tokenizer\" : tokenizer}\n",
    ")\n",
    "splitted_dataset = full_dataset.train_test_split(0.2)\n",
    "document_train_set = splitted_dataset[\"train\"]\n",
    "document_valid_set = splitted_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "R=4\n",
    "K=2\n",
    "B=32\n",
    "E=20\n",
    "VOCSIZE=tokenizer.vocab_size\n",
    "D=200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CustomDataset(document_train_set, R)\n",
    "valid_set = CustomDataset(document_valid_set, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn_with_params = functools.partial(collate_fn, R=R, K=K, VOCSIZE=VOCSIZE)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_set, batch_size=batch_size, collate_fn=collate_fn_with_params\n",
    ")   \n",
    "valid_dataloader = DataLoader(\n",
    "    valid_set, batch_size=batch_size, collate_fn=collate_fn_with_params\n",
    ")\n",
    "n_valid = len(valid_set)\n",
    "n_train = len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Word2Vec(\n",
       "  (emb): Embedding(30522, 200, padding_idx=0)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(VOCSIZE, D)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [00:20<00:00, 60.87it/s]\n",
      "100%|██████████| 313/313 [00:01<00:00, 191.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      "\t - Train loss: 2.5815 Train acc: 73.0883 Val loss: 1.3479 Val acc:77.1193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [00:20<00:00, 62.31it/s]\n",
      "100%|██████████| 313/313 [00:01<00:00, 195.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      "\t - Train loss: 1.1259 Train acc: 78.0768 Val loss: 1.0163 Val acc:78.4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [00:20<00:00, 62.39it/s]\n",
      "100%|██████████| 313/313 [00:01<00:00, 193.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 \n",
      "\t - Train loss: 0.9397 Train acc: 78.7971 Val loss: 0.9148 Val acc:78.8093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [00:20<00:00, 61.72it/s]\n",
      "100%|██████████| 313/313 [00:01<00:00, 197.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 \n",
      "\t - Train loss: 0.8745 Train acc: 79.1941 Val loss: 0.8708 Val acc:79.1362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [00:19<00:00, 62.81it/s]\n",
      "100%|██████████| 313/313 [00:01<00:00, 201.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 \n",
      "\t - Train loss: 0.8441 Train acc: 79.4370 Val loss: 0.8487 Val acc:79.3310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [00:19<00:00, 62.92it/s]\n",
      "100%|██████████| 313/313 [00:01<00:00, 196.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 \n",
      "\t - Train loss: 0.8272 Train acc: 79.6111 Val loss: 0.8372 Val acc:79.4853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [00:19<00:00, 62.57it/s]\n",
      "100%|██████████| 313/313 [00:01<00:00, 193.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 \n",
      "\t - Train loss: 0.8165 Train acc: 79.7632 Val loss: 0.8308 Val acc:79.5851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [00:20<00:00, 62.47it/s]\n",
      "100%|██████████| 313/313 [00:01<00:00, 198.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 \n",
      "\t - Train loss: 0.8095 Train acc: 79.8662 Val loss: 0.8281 Val acc:79.6396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [00:20<00:00, 62.43it/s]\n",
      "100%|██████████| 313/313 [00:01<00:00, 197.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 \n",
      "\t - Train loss: 0.8043 Train acc: 79.9392 Val loss: 0.8270 Val acc:79.6743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [00:20<00:00, 62.46it/s]\n",
      "100%|██████████| 313/313 [00:01<00:00, 196.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 \n",
      "\t - Train loss: 0.8004 Train acc: 79.9953 Val loss: 0.8270 Val acc:79.6867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [00:20<00:00, 62.49it/s]\n",
      "100%|██████████| 313/313 [00:01<00:00, 193.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 \n",
      "\t - Train loss: 0.7973 Train acc: 80.0428 Val loss: 0.8270 Val acc:79.7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [00:20<00:00, 62.42it/s]\n",
      "100%|██████████| 313/313 [00:01<00:00, 189.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 \n",
      "\t - Train loss: 0.7948 Train acc: 80.0849 Val loss: 0.8269 Val acc:79.7230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [00:20<00:00, 60.77it/s]\n",
      "100%|██████████| 313/313 [00:01<00:00, 175.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 \n",
      "\t - Train loss: 0.7930 Train acc: 80.1189 Val loss: 0.8272 Val acc:79.7302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [00:21<00:00, 59.11it/s]\n",
      "100%|██████████| 313/313 [00:01<00:00, 160.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 \n",
      "\t - Train loss: 0.7916 Train acc: 80.1388 Val loss: 0.8272 Val acc:79.7399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [00:20<00:00, 60.06it/s]\n",
      "100%|██████████| 313/313 [00:01<00:00, 180.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 \n",
      "\t - Train loss: 0.7905 Train acc: 80.1580 Val loss: 0.8274 Val acc:79.7379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [00:20<00:00, 62.13it/s]\n",
      "100%|██████████| 313/313 [00:01<00:00, 193.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 \n",
      "\t - Train loss: 0.7896 Train acc: 80.1703 Val loss: 0.8281 Val acc:79.7359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [00:20<00:00, 61.18it/s]\n",
      "100%|██████████| 313/313 [00:01<00:00, 185.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 \n",
      "\t - Train loss: 0.7889 Train acc: 80.1871 Val loss: 0.8282 Val acc:79.7377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [00:20<00:00, 61.86it/s]\n",
      "100%|██████████| 313/313 [00:01<00:00, 191.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 \n",
      "\t - Train loss: 0.7882 Train acc: 80.2038 Val loss: 0.8286 Val acc:79.7589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [00:19<00:00, 62.98it/s]\n",
      "100%|██████████| 313/313 [00:01<00:00, 197.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 \n",
      "\t - Train loss: 0.7877 Train acc: 80.2118 Val loss: 0.8284 Val acc:79.7736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [00:20<00:00, 62.33it/s]\n",
      "100%|██████████| 313/313 [00:01<00:00, 201.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 \n",
      "\t - Train loss: 0.7872 Train acc: 80.2239 Val loss: 0.8288 Val acc:79.7770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lr=5e-3\n",
    "writer = SummaryWriter(\"runs/Word2Vec_v1\")\n",
    "\n",
    "list_train_loss, list_train_acc, list_val_loss, list_val_acc = training(\n",
    "    model=model, \n",
    "    lr=lr,\n",
    "    E=E, \n",
    "    B=B, \n",
    "    loss_fn=MyLoss, \n",
    "    train_dataloader=train_dataloader, \n",
    "    valid_dataloader=valid_dataloader,\n",
    "    writer=writer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list_train_acc, label=\"train\")\n",
    "plt.plot(list_val_acc, label=\"val\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.legend()\n",
    "plt.xticks(range(0,E,3));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performance plateaus at 80% accuracy, it overfits after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'positive_similarity': tensor([[[[3.7371, 3.8635, 3.1721, 2.0185, 4.7882, 2.6421, 9.8530]]]],\n",
      "       device='cuda:0'), 'negative_similarity': tensor([[[[-1.6685,  0.0181,  0.0000,  2.4596, -5.7207, -5.7207]]]],\n",
      "       device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "print(test_similarity(\n",
    "    model, \n",
    "    w=\"actor\", \n",
    "    Cplus=[\"bad\",\"good\",\"terrible\",\"handsome\",\"great\",\"absolutely\", \"actor\"], \n",
    "    Cminus=[\"mathematician\",\"robotic\",\"[PAD]\",\"?\",\"Newton\",\"stochastic\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a good model should have negative values for words in $C^-$ and positive for $C^+$. Interestingly enough, the token \"[PAD]\", has a null dot product with every word. This is because we used it as a placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, root=\"runs/Word2Vec_v1/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try different things : \n",
    "- for tokens at the beginning and end of the reviews, we will add the token itself as their positive context instead of \"[PAD]\". <br>\n",
    "  For example the review \"I liked this movie\" should have (for the word \"I\" and R=2) $C^+ =$  [\"I\", \"I\", \"I\", \"liked\", \"this\"]. <br>\n",
    "  So that the dot product of a token with itself is high. Using \"[PAD]\" forces tokens at the beginning of the review to have a high similarity with \"[PAD]\", which is not necessarily what we want.\n",
    "- Reduce the embedding dimension. As we've seen, there is a little bit of overfitting, reducing the size of the embeddings will simplify the model a little bit and reduce overfitting.\n",
    "- Reduce the learning rate with a scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_filepath):\n",
    "    embeddings_weights = torch.load(model_filepath)\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
