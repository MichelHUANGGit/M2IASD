model:
  name: tinyllama1B
  alpha: 1.0
  target_layers_rank: 
    self_attn.q_proj: 2
    self_attn.k_proj: 2
    self_attn.v_proj: 2
    self_attn.o_proj: 2

dataset:
  name: tuetschek
  max_length: 256
  # name: hellaswag
  # max_length: 192

# 90% of tuetschek inputs are of length <400. use max_length=256 or 384
# hellaswag inputs are of length < 192. use max_length=192

training:
  precision: high
  batch_size: 32
  grad_accum_steps: 4
  epochs: 1.5
  warmup_epochs: 0.5
  eval_every_x_epoch: 0.8
  save_every_x_epoch: 0.8
  use_compile: true
  use_autocast: false #FIXME: doesn't work with compile during flash attn
  weight_decay: 0.10
  use_cosine_decay: false
  max_lr: 5.0e-04
  min_lr: 5.0e-05
  lr_decay: -0.5
  beta1: 0.90
  beta2: 0.98
  max_grad_norm: 5.0
