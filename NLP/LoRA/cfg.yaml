model:
  name: tinyllama1B_e2e_nlg
  alpha: 1.0
  target_layers_rank:
    mlp.gate_proj: 8
    mlp.up_proj: 8
    mlp.down_proj: 8

training:
  precision: high
  batch_size: 32
  grad_accum_steps: 4
  epochs: 2.0
  warmup_epochs: 0.5
  eval_every_x_epoch: 0.7
  save_every_x_epoch: 0.7
  use_compile: true
  use_autocast: false #FIXME: doesn't work with compile during flash attn
  weight_decay: 0.10
  use_lr_scheduler: true
  max_lr: 0.0001
  min_lr: 5.0e-05
  beta1: 0.95
  beta2: 0.99
  max_grad_norm: 1.0
